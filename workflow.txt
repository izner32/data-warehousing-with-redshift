// WORKFLOW 0 - FILE GUIDE
table_creation.py - for coding the data mart(fact and set of dim table, place where we'll do analysis)
table_queries.py - contains the queries for creating databases,dropping databases, creating tables for star schemas, and inserting values at tables 
etl.ipynb - extract data from source, transform and clean it, load it into data mart
dwh.cfg - values to be used for connecting to aws | config file is pretty much just like an env file where you place your variables in another file except in env file its secret in config its not secret values | this file is extremely optional tbh 
iac_redshift-s3 - iac(infrastructure as code) - creating redshift cluster, cluster == database and s3 bucket 

// WORKFLOW 1 - HIGH LEVEL WORKFLOW 
- design dimensional model (schema) for data mart | this is where data scientist and analysts would query from 
- code the data mart 
    - create database for postgres
    - drop and create table for fresh new tables 
- code creation of redshift cluster and s3 and iam role/policy
    - create an iam user thru gui (aws website) | iam is an access management - to manage the level of authorization a user has over aws services | giving each user (business analyst,admin,etc.) or group(collection of user) or an aws service(ec2,s3,etc.) a policy/permission(things they can access) or a role(collection of policy/permission)
        - create a role/policy
    - create s3 bucket thru cli | s3: like a directory where it contain directory/ies and/or file/s
        - extract from main source -> transform -> put data in s3 
    - create redshift cluster thru cli | redshift cluster: data warehouse  
        - attach a role/policy into redshift
- etl and staging area 
    - extract from s3 
    - transform 
    - staging area: 
        - load data from s3 into redshift by using copy command | purpose of this is because you could easily query from redshift 
    - load set of dimensional table(which we're gonna do analysis from) by querying from redshit (it has data from the main source) then insert data to tables 
- make an analysis from this set of table (data mart)  


// WORKFLOW 2 - LOW LEVEL WORKFLOW 
- design dimensional model (schema) for data warehouse
- code the data mart 
    - create database for postgres
    - drop and create table for fresh new tables 
- code creation of redshift cluster(data warehouse in cloud) 
    - do this inside aws website 
        - create an iam user - to give access into using different aws services + attach a policy to that iam user 
        - use the access key id and secret access key from the iam user 
    - do this as iac(infrastructure as code)
        - creating a role for iam 
        - attach a role policy 
        - create a redshift client then create a cluster, make use of the role you just created 
        - open an incoming tcp port to access the cluster endpoint - this is to connect to redshift 
        - delete cluster as its expensive, and we just finished our demo 
- etl ,staging area
    - data extraction 
    - etl 
        - do transformation 
        - put transformed and cleaned data into s3 
    - staging area 
        - load s3 data into redshift by using COPY command (redshift is the data warehouse)
    - continue etl 
        - load data mart with data from redshift - you have to do a few transformation to fit the data you wanted to data mart
- analysis on data marts 
